#!/usr/bin/env python3
"""
æ™ºèƒ½åŒ– Monad ç´¢å¼•å™¨å¯åŠ¨è„šæœ¬
åŸºäºRPCé™åˆ¶æµ‹è¯•ç»“æœï¼ŒåŠ¨æ€è°ƒæ•´å‚æ•°ï¼Œè‡ªåŠ¨é‡è¯•æœºåˆ¶
"""

import argparse
import logging
import os
import sys
import time
import subprocess
import requests
import json
from datetime import datetime
from pathlib import Path

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'monad_indexer_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class MonadIndexerOptimized:
    """ä¼˜åŒ–çš„ Monad ç´¢å¼•å™¨"""
    
    def __init__(self):
        self.rpc_url = "https://testnet-rpc.monad.xyz"
        self.postgres_uri = "postgresql://devuser:hemera123@localhost:5432/hemera_indexer"
        self.config_file = "config/indexer-config-monad-optimized.yaml"
        
        # åŸºäºæµ‹è¯•ç»“æœçš„å‚æ•°
        self.optimal_params = {
            'batch_size': 100,
            'max_workers': 8,
            'block_batch_size': 30,
            'timeout': 45,
            'retry_attempts': 5,
            'retry_delay': 2
        }
        
        # åŠ¨æ€è°ƒæ•´å‚æ•°
        self.current_params = self.optimal_params.copy()
        self.performance_stats = {
            'total_requests': 0,
            'failed_requests': 0,
            'avg_latency': 0,
            'error_rate': 0
        }
    
    def test_rpc_connection(self):
        """æµ‹è¯•RPCè¿æ¥å’Œæ€§èƒ½"""
        logger.info("ğŸ” æµ‹è¯• Monad RPC è¿æ¥...")
        
        payload = {
            "jsonrpc": "2.0",
            "method": "eth_blockNumber",
            "params": [],
            "id": 1
        }
        
        try:
            start_time = time.time()
            response = requests.post(self.rpc_url, json=payload, timeout=10)
            latency = time.time() - start_time
            
            if response.status_code == 200:
                result = response.json()
                latest_block = int(result['result'], 16)
                logger.info(f"âœ… RPCè¿æ¥æˆåŠŸ, æœ€æ–°åŒºå—: {latest_block}, å»¶è¿Ÿ: {latency:.3f}s")
                return True, latest_block, latency
            else:
                logger.error(f"âŒ RPCè¿æ¥å¤±è´¥, çŠ¶æ€ç : {response.status_code}")
                return False, 0, 0
                
        except Exception as e:
            logger.error(f"âŒ RPCè¿æ¥å¼‚å¸¸: {e}")
            return False, 0, 0
    
    def test_concurrent_performance(self):
        """æµ‹è¯•å¹¶å‘æ€§èƒ½å¹¶åŠ¨æ€è°ƒæ•´å‚æ•°"""
        logger.info("ğŸ§ª æµ‹è¯•å¹¶å‘æ€§èƒ½...")
        
        import asyncio
        import aiohttp
        
        async def test_concurrent_requests(concurrent_count):
            """å¼‚æ­¥æµ‹è¯•å¹¶å‘è¯·æ±‚"""
            payload = {
                "jsonrpc": "2.0",
                "method": "eth_blockNumber",
                "params": [],
                "id": 1
            }
            
            async def make_request(session, request_id):
                try:
                    start = time.time()
                    async with session.post(self.rpc_url, json={**payload, "id": request_id}) as response:
                        latency = time.time() - start
                        return {"success": response.status == 200, "latency": latency}
                except Exception:
                    return {"success": False, "latency": 0}
            
            async with aiohttp.ClientSession() as session:
                tasks = [make_request(session, i) for i in range(concurrent_count)]
                results = await asyncio.gather(*tasks)
            
            return results
        
        # æµ‹è¯•ä¸åŒå¹¶å‘çº§åˆ«
        for concurrent in [5, 8, 10, 15]:
            results = asyncio.run(test_concurrent_requests(concurrent))
            successful = [r for r in results if r['success']]
            success_rate = len(successful) / len(results)
            
            logger.info(f"   å¹¶å‘ {concurrent}: æˆåŠŸç‡ {success_rate:.2%}")
            
            if success_rate >= 0.95:  # 95%æˆåŠŸç‡
                self.current_params['max_workers'] = concurrent
            else:
                break
        
        logger.info(f"ğŸ“Š åŠ¨æ€è°ƒæ•´åçš„æœ€å¤§å·¥ä½œçº¿ç¨‹: {self.current_params['max_workers']}")
    
    def check_database_connection(self):
        """æ£€æŸ¥æ•°æ®åº“è¿æ¥"""
        logger.info("ğŸ” æ£€æŸ¥æ•°æ®åº“è¿æ¥...")
        
        try:
            import psycopg2
            conn = psycopg2.connect(self.postgres_uri)
            conn.close()
            logger.info("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            return False
    
    def build_indexer_command(self, start_block, end_block):
        """æ„å»ºç´¢å¼•å™¨å‘½ä»¤"""
        cmd = [
            sys.executable, "hemera.py", "stream",
            "--provider-uri", self.rpc_url,
            "--debug-provider-uri", self.rpc_url,
            "--output", f"postgres+{self.postgres_uri}",
            "--start-block", str(start_block),
            "--end-block", str(end_block),
            "--config", self.config_file,
            
            # åŠ¨æ€è°ƒæ•´çš„å‚æ•°
            "--entity-types", "block,transaction,log,token_transfer,trace,contract",
            "--block-batch-size", str(self.current_params['block_batch_size']),
            "--batch-size", str(self.current_params['batch_size']),
            "--max-workers", str(self.current_params['max_workers']),
            "--timeout", str(self.current_params['timeout']),
            "--retry-attempts", str(self.current_params['retry_attempts']),
            "--retry-delay", str(self.current_params['retry_delay']),
            
            # ç›‘æ§å’Œæ—¥å¿—
            "--log-level", "INFO",
            "--enable-performance-logging",
            "--log-rpc-calls"
        ]
        
        return cmd
    
    def run_indexer_with_retry(self, start_block, end_block, max_retries=3):
        """è¿è¡Œç´¢å¼•å™¨ï¼ŒåŒ…å«æ™ºèƒ½é‡è¯•æœºåˆ¶"""
        logger.info(f"ğŸš€ å¼€å§‹ç´¢å¼•åŒºå— {start_block} - {end_block}")
        
        for attempt in range(max_retries):
            try:
                cmd = self.build_indexer_command(start_block, end_block)
                logger.info(f"ğŸ“‹ å°è¯• {attempt + 1}/{max_retries}")
                
                # æ‰§è¡Œå‘½ä»¤
                process = subprocess.run(cmd, capture_output=True, text=True, timeout=3600)
                
                if process.returncode == 0:
                    logger.info("âœ… ç´¢å¼•å™¨æ‰§è¡ŒæˆåŠŸ")
                    return True
                else:
                    logger.error(f"âŒ ç´¢å¼•å™¨æ‰§è¡Œå¤±è´¥ï¼Œè¿”å›ç : {process.returncode}")
                    logger.error(f"é”™è¯¯è¾“å‡º: {process.stderr}")
                    
                    # æ ¹æ®é”™è¯¯è°ƒæ•´å‚æ•°
                    if "429" in process.stderr or "rate limit" in process.stderr.lower():
                        self.adjust_params_for_rate_limit()
                    elif "timeout" in process.stderr.lower():
                        self.adjust_params_for_timeout()
                    
            except subprocess.TimeoutExpired:
                logger.error("âŒ ç´¢å¼•å™¨æ‰§è¡Œè¶…æ—¶")
                self.adjust_params_for_timeout()
            except Exception as e:
                logger.error(f"âŒ ç´¢å¼•å™¨æ‰§è¡Œå¼‚å¸¸: {e}")
            
            if attempt < max_retries - 1:
                wait_time = (attempt + 1) * 10
                logger.info(f"â³ ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                time.sleep(wait_time)
        
        return False
    
    def adjust_params_for_rate_limit(self):
        """è°ƒæ•´å‚æ•°åº”å¯¹é€Ÿç‡é™åˆ¶"""
        logger.info("ğŸ”§ æ£€æµ‹åˆ°é€Ÿç‡é™åˆ¶ï¼Œè°ƒæ•´å‚æ•°...")
        self.current_params['batch_size'] = max(50, self.current_params['batch_size'] // 2)
        self.current_params['max_workers'] = max(2, self.current_params['max_workers'] // 2)
        self.current_params['retry_delay'] = min(10, self.current_params['retry_delay'] * 2)
        logger.info(f"   è°ƒæ•´åå‚æ•°: batch_size={self.current_params['batch_size']}, "
                   f"max_workers={self.current_params['max_workers']}, "
                   f"retry_delay={self.current_params['retry_delay']}")
    
    def adjust_params_for_timeout(self):
        """è°ƒæ•´å‚æ•°åº”å¯¹è¶…æ—¶"""
        logger.info("ğŸ”§ æ£€æµ‹åˆ°è¶…æ—¶ï¼Œè°ƒæ•´å‚æ•°...")
        self.current_params['timeout'] = min(120, self.current_params['timeout'] * 1.5)
        self.current_params['block_batch_size'] = max(10, self.current_params['block_batch_size'] // 2)
        logger.info(f"   è°ƒæ•´åå‚æ•°: timeout={self.current_params['timeout']}, "
                   f"block_batch_size={self.current_params['block_batch_size']}")
    
    def run(self, start_block=1, end_block=100):
        """ä¸»æ‰§è¡Œå‡½æ•°"""
        logger.info("ğŸš€ æ™ºèƒ½åŒ– Monad ç´¢å¼•å™¨å¯åŠ¨")
        logger.info("=" * 50)
        
        # é¢„æ£€æŸ¥
        rpc_ok, latest_block, latency = self.test_rpc_connection()
        if not rpc_ok:
            logger.error("âŒ RPCè¿æ¥å¤±è´¥ï¼Œé€€å‡º")
            return False
        
        if not self.check_database_connection():
            logger.error("âŒ æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œé€€å‡º")
            return False
        
        # åŠ¨æ€è°ƒæ•´å‚æ•°
        self.test_concurrent_performance()
        
        # è°ƒæ•´ç»“æŸåŒºå—
        if end_block > latest_block:
            end_block = latest_block
            logger.info(f"âš ï¸ è°ƒæ•´ç»“æŸåŒºå—ä¸ºæœ€æ–°åŒºå—: {end_block}")
        
        # åˆ†æ‰¹å¤„ç†å¤§èŒƒå›´åŒºå—
        chunk_size = 1000  # æ¯æ‰¹å¤„ç†1000ä¸ªåŒºå—
        current_start = start_block
        
        while current_start <= end_block:
            current_end = min(current_start + chunk_size - 1, end_block)
            
            success = self.run_indexer_with_retry(current_start, current_end)
            if not success:
                logger.error(f"âŒ åŒºå— {current_start}-{current_end} ç´¢å¼•å¤±è´¥")
                return False
            
            current_start = current_end + 1
            
            # çŸ­æš‚ä¼‘æ¯é¿å…è¿‡è½½
            if current_start <= end_block:
                time.sleep(5)
        
        logger.info("âœ… æ‰€æœ‰åŒºå—ç´¢å¼•å®Œæˆ")
        return True

def main():
    parser = argparse.ArgumentParser(description="æ™ºèƒ½åŒ– Monad ç´¢å¼•å™¨")
    parser.add_argument("--start-block", type=int, default=1, help="èµ·å§‹åŒºå—å·")
    parser.add_argument("--end-block", type=int, default=100, help="ç»“æŸåŒºå—å·")
    
    args = parser.parse_args()
    
    indexer = MonadIndexerOptimized()
    success = indexer.run(args.start_block, args.end_block)
    
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()